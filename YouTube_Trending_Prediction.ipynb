{"cells":[{"metadata":{},"cell_type":"markdown","source":"<i>YT Trending Prediction</i><br>\n--\n\nCoded by :     \n* Khevin Pandapotan (No GitHub)\n* sammyon7"},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom datetime import datetime\nimport os\nfrom sklearn.preprocessing import StandardScaler","execution_count":1,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Dataset <a href=\"https://www.kaggle.com/datasnaek/youtube-new\">link</a>"},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"filepath1=\"/kaggle/input/youtube-new/US_category_id.json\"\nfilepath2=\"/kaggle/input/youtube-new/USvideos.csv\"\ncategory_id_df = pd.read_json(filepath1)\nvideos_df = pd.read_csv(filepath2,header='infer')","execution_count":3,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def clean_video_csv(video_df,country_code):\n    video_df[\"tags\"] = video_df[\"tags\"].apply(lambda x:x.replace('\"',\"\"))\n    video_df[\"title\"] = video_df[\"title\"].apply(lambda x:x.replace(',',' '))\n    video_df[\"channel_title\"] = video_df[\"channel_title\"].apply(lambda x:x.replace(',',' '))\n    video_df[\"description\"] = video_df[\"description\"].apply(lambda x:str(x).replace('\\r',''))\n    video_df[\"description\"] = video_df[\"description\"].apply(lambda x:str(x).replace(',',' '))\n    video_df[\"description\"] = video_df[\"description\"].apply(lambda x:str(x).replace('\"',''))\n    video_df[\"country\"] = country_code\n    return video_df\n\ncountry_code=['US']\nfor c in country_code:\n    filepath=\"/kaggle/input/youtube-new/\"+c+\"videos.csv\"\n    video_df = pd.read_csv(filepath,header='infer')\n    savepath = \"/kaggle/working/\"+c+\"videos1.csv\"\n    video_df = clean_video_csv(video_df,c)\n    video_df.to_csv(savepath,index=False)","execution_count":7,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def category_extract (df,country_code):\n    category_id = []\n    category_title = []\n    for i in range(df.shape[0]):\n        category_id.append(df.iloc[i][\"items\"]['id'])\n        category_title.append(df.iloc[i][\"items\"][\"snippet\"][\"title\"])\n    category_df = pd.DataFrame()\n    category_df[\"category_id\"] = category_id\n    category_df[\"category_title\"] = category_title\n    category_df.insert(category_df.shape[1],\"country_code\",country_code)\n    return category_df\n\ncategory_all = pd.DataFrame()\nfor c in country_code:\n    filepath=\"/kaggle/input/youtube-new/\"+c+\"_category_id.json\"\n    category_id_df = pd.read_json(filepath)\n    category_all = pd.concat([category_all,category_extract(category_id_df,c)])\n    \nsavepath = \"/kaggle/working/category_all.csv\"\ncategory_all.to_csv(savepath,index=False)","execution_count":9,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"US = pd.read_csv(\"/kaggle/working/USvideos1.csv\")\ncategory = pd.read_csv(\"/kaggle/working/category_all.csv\")\nUS1 =US.merge(category,how=\"inner\",left_on=[\"category_id\",\"country\"],right_on=[\"category_id\",\"country_code\"])\nUS1[\"trending_date1\"] = US1[\"trending_date\"].apply(lambda x: pd.Timestamp(int(\"20\"+x[0:2]),int(x[-2:]),int(x[3:5]),0))","execution_count":10,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"columns = [\"video_id\",\"trending_date1\",\"channel_title\",\"publish_time\",\"views\",\"likes\",\"dislikes\",\"comment_count\",\"category_title\"]\nUS1 = US1[columns].copy()","execution_count":11,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"US1.head(10)","execution_count":13,"outputs":[{"output_type":"execute_result","execution_count":13,"data":{"text/plain":"      video_id trending_date1                   channel_title  \\\n0  2kyS6SvSYSE     2017-11-14                    CaseyNeistat   \n1  0mlNzVSJrT0     2017-11-14                         Nobrand   \n2  STI2fI7sKMo     2017-11-14              Shawn Johnson East   \n3  KODzih-pYlU     2017-11-14                    Grace Helbig   \n4  8mhTWqWlQzU     2017-11-14                  Safiya Nygaard   \n5  pa_oUisZZy0     2017-11-14                  BuzzFeed Celeb   \n6  fCTKDn3Q8xQ     2017-11-14                  Rachel and Jun   \n7  _dhneCO4YEE     2017-11-14  Full Frontal with Samantha Bee   \n8  EYkEshCOhEU     2017-11-14                Nicole Guerriero   \n9  Eg_kW5fw6qU     2017-11-14             SuperCarlinBrothers   \n\n               publish_time    views   likes  dislikes  comment_count  \\\n0  2017-11-13T17:13:01.000Z   748374   57527      2966          15954   \n1  2017-04-21T06:47:32.000Z    98966    2486       184            532   \n2  2017-11-11T15:00:03.000Z   321053    4451      1772            895   \n3  2017-11-11T18:08:04.000Z   197062    7250       217            456   \n4  2017-11-11T01:19:33.000Z  2744430  115426      1110           6541   \n5  2017-11-11T00:30:16.000Z   177707    6271        88            275   \n6  2017-11-09T11:24:14.000Z  1098897   43875      1326           4702   \n7  2017-11-09T07:00:01.000Z   362009    5505      2356           1264   \n8  2017-11-10T00:24:29.000Z   294387   15247       385            976   \n9  2017-11-09T23:27:18.000Z   188003    9091       135           2814   \n\n   category_title  \n0  People & Blogs  \n1  People & Blogs  \n2  People & Blogs  \n3  People & Blogs  \n4  People & Blogs  \n5  People & Blogs  \n6  People & Blogs  \n7  People & Blogs  \n8  People & Blogs  \n9  People & Blogs  ","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>video_id</th>\n      <th>trending_date1</th>\n      <th>channel_title</th>\n      <th>publish_time</th>\n      <th>views</th>\n      <th>likes</th>\n      <th>dislikes</th>\n      <th>comment_count</th>\n      <th>category_title</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2kyS6SvSYSE</td>\n      <td>2017-11-14</td>\n      <td>CaseyNeistat</td>\n      <td>2017-11-13T17:13:01.000Z</td>\n      <td>748374</td>\n      <td>57527</td>\n      <td>2966</td>\n      <td>15954</td>\n      <td>People &amp; Blogs</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0mlNzVSJrT0</td>\n      <td>2017-11-14</td>\n      <td>Nobrand</td>\n      <td>2017-04-21T06:47:32.000Z</td>\n      <td>98966</td>\n      <td>2486</td>\n      <td>184</td>\n      <td>532</td>\n      <td>People &amp; Blogs</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>STI2fI7sKMo</td>\n      <td>2017-11-14</td>\n      <td>Shawn Johnson East</td>\n      <td>2017-11-11T15:00:03.000Z</td>\n      <td>321053</td>\n      <td>4451</td>\n      <td>1772</td>\n      <td>895</td>\n      <td>People &amp; Blogs</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>KODzih-pYlU</td>\n      <td>2017-11-14</td>\n      <td>Grace Helbig</td>\n      <td>2017-11-11T18:08:04.000Z</td>\n      <td>197062</td>\n      <td>7250</td>\n      <td>217</td>\n      <td>456</td>\n      <td>People &amp; Blogs</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>8mhTWqWlQzU</td>\n      <td>2017-11-14</td>\n      <td>Safiya Nygaard</td>\n      <td>2017-11-11T01:19:33.000Z</td>\n      <td>2744430</td>\n      <td>115426</td>\n      <td>1110</td>\n      <td>6541</td>\n      <td>People &amp; Blogs</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>pa_oUisZZy0</td>\n      <td>2017-11-14</td>\n      <td>BuzzFeed Celeb</td>\n      <td>2017-11-11T00:30:16.000Z</td>\n      <td>177707</td>\n      <td>6271</td>\n      <td>88</td>\n      <td>275</td>\n      <td>People &amp; Blogs</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>fCTKDn3Q8xQ</td>\n      <td>2017-11-14</td>\n      <td>Rachel and Jun</td>\n      <td>2017-11-09T11:24:14.000Z</td>\n      <td>1098897</td>\n      <td>43875</td>\n      <td>1326</td>\n      <td>4702</td>\n      <td>People &amp; Blogs</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>_dhneCO4YEE</td>\n      <td>2017-11-14</td>\n      <td>Full Frontal with Samantha Bee</td>\n      <td>2017-11-09T07:00:01.000Z</td>\n      <td>362009</td>\n      <td>5505</td>\n      <td>2356</td>\n      <td>1264</td>\n      <td>People &amp; Blogs</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>EYkEshCOhEU</td>\n      <td>2017-11-14</td>\n      <td>Nicole Guerriero</td>\n      <td>2017-11-10T00:24:29.000Z</td>\n      <td>294387</td>\n      <td>15247</td>\n      <td>385</td>\n      <td>976</td>\n      <td>People &amp; Blogs</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>Eg_kW5fw6qU</td>\n      <td>2017-11-14</td>\n      <td>SuperCarlinBrothers</td>\n      <td>2017-11-09T23:27:18.000Z</td>\n      <td>188003</td>\n      <td>9091</td>\n      <td>135</td>\n      <td>2814</td>\n      <td>People &amp; Blogs</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"trendingdate_df = US1.groupby(\"video_id\").trending_date1.describe(datetime_is_numeric=True).reset_index()\nvideos = trendingdate_df[trendingdate_df[\"count\"].values>4].video_id\nUS2 = US1[ US1.video_id.isin(videos.values)]","execution_count":14,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(trendingdate_df.head(10))","execution_count":15,"outputs":[{"output_type":"stream","text":"      video_id count                mean        min                 25%  \\\n0  -0CMnp02rNY     6 2018-06-08 12:00:00 2018-06-06 2018-06-07 06:00:00   \n1  -0NYY8cqdiQ     1 2018-02-01 00:00:00 2018-02-01 2018-02-01 00:00:00   \n2  -1Hm41N0dUs     3 2018-04-30 00:00:00 2018-04-29 2018-04-29 12:00:00   \n3  -1yT-K3c6YI     4 2017-11-30 12:00:00 2017-11-29 2017-11-29 18:00:00   \n4  -2RVw2_QyxQ     3 2017-11-15 00:00:00 2017-11-14 2017-11-14 12:00:00   \n5  -2aVkGcI7ZA     4 2018-04-28 12:00:00 2018-04-27 2018-04-27 18:00:00   \n6  -2b4qSoMnKE     2 2017-12-20 12:00:00 2017-12-20 2017-12-20 06:00:00   \n7  -2wRFv-mScQ     4 2018-02-15 12:00:00 2018-02-14 2018-02-14 18:00:00   \n8  -35jibKqbEo     8 2018-02-18 12:00:00 2018-02-15 2018-02-16 18:00:00   \n9  -37nIo_tLnk     8 2017-12-29 12:00:00 2017-12-26 2017-12-27 18:00:00   \n\n                  50%                 75%        max  \n0 2018-06-08 12:00:00 2018-06-09 18:00:00 2018-06-11  \n1 2018-02-01 00:00:00 2018-02-01 00:00:00 2018-02-01  \n2 2018-04-30 00:00:00 2018-04-30 12:00:00 2018-05-01  \n3 2017-11-30 12:00:00 2017-12-01 06:00:00 2017-12-02  \n4 2017-11-15 00:00:00 2017-11-15 12:00:00 2017-11-16  \n5 2018-04-28 12:00:00 2018-04-29 06:00:00 2018-04-30  \n6 2017-12-20 12:00:00 2017-12-20 18:00:00 2017-12-21  \n7 2018-02-15 12:00:00 2018-02-16 06:00:00 2018-02-17  \n8 2018-02-18 12:00:00 2018-02-20 06:00:00 2018-02-22  \n9 2017-12-29 12:00:00 2017-12-31 06:00:00 2018-01-02  \n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"def standardize(data):\n    scaler = StandardScaler()\n    scaler = scaler.fit(data)\n    transformed = scaler.transform(data)\n    return scaler,transformed","execution_count":18,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"scaler_views, US_views = standardize(US2.views.values.reshape(-1,1))","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"US3 = pd.DataFrame()\nUS3[\"trending_date1\"] = US2[\"trending_date1\"]\nUS3[\"video_id\"] = US2[\"video_id\"]\nUS3[\"views\"] = US_views\nUS3.reset_index(inplace=True)","execution_count":24,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(US3.head(10))","execution_count":25,"outputs":[{"output_type":"stream","text":"   index trending_date1     video_id     views\n0      0     2017-11-14  2kyS6SvSYSE -0.237504\n1     17     2017-11-15  2kyS6SvSYSE -0.055430\n2     31     2017-11-16  cmoknv58jjE -0.104086\n3     32     2017-11-16  2kyS6SvSYSE -0.038156\n4     33     2017-11-16  Jidk0O6uu-0 -0.325464\n5     42     2017-11-17  t1shnJT8NCY -0.281801\n6     44     2017-11-17  cmoknv58jjE -0.062990\n7     46     2017-11-17  Jidk0O6uu-0 -0.323873\n8     47     2017-11-17  2kyS6SvSYSE -0.028610\n9     54     2017-11-18  t1shnJT8NCY -0.265967\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"US3.drop(\"index\",axis=1,inplace=True)\nUS3.head(10)","execution_count":26,"outputs":[{"output_type":"execute_result","execution_count":26,"data":{"text/plain":"  trending_date1     video_id     views\n0     2017-11-14  2kyS6SvSYSE -0.237504\n1     2017-11-15  2kyS6SvSYSE -0.055430\n2     2017-11-16  cmoknv58jjE -0.104086\n3     2017-11-16  2kyS6SvSYSE -0.038156\n4     2017-11-16  Jidk0O6uu-0 -0.325464\n5     2017-11-17  t1shnJT8NCY -0.281801\n6     2017-11-17  cmoknv58jjE -0.062990\n7     2017-11-17  Jidk0O6uu-0 -0.323873\n8     2017-11-17  2kyS6SvSYSE -0.028610\n9     2017-11-18  t1shnJT8NCY -0.265967","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>trending_date1</th>\n      <th>video_id</th>\n      <th>views</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>2017-11-14</td>\n      <td>2kyS6SvSYSE</td>\n      <td>-0.237504</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2017-11-15</td>\n      <td>2kyS6SvSYSE</td>\n      <td>-0.055430</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>2017-11-16</td>\n      <td>cmoknv58jjE</td>\n      <td>-0.104086</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>2017-11-16</td>\n      <td>2kyS6SvSYSE</td>\n      <td>-0.038156</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>2017-11-16</td>\n      <td>Jidk0O6uu-0</td>\n      <td>-0.325464</td>\n    </tr>\n    <tr>\n      <th>5</th>\n      <td>2017-11-17</td>\n      <td>t1shnJT8NCY</td>\n      <td>-0.281801</td>\n    </tr>\n    <tr>\n      <th>6</th>\n      <td>2017-11-17</td>\n      <td>cmoknv58jjE</td>\n      <td>-0.062990</td>\n    </tr>\n    <tr>\n      <th>7</th>\n      <td>2017-11-17</td>\n      <td>Jidk0O6uu-0</td>\n      <td>-0.323873</td>\n    </tr>\n    <tr>\n      <th>8</th>\n      <td>2017-11-17</td>\n      <td>2kyS6SvSYSE</td>\n      <td>-0.028610</td>\n    </tr>\n    <tr>\n      <th>9</th>\n      <td>2017-11-18</td>\n      <td>t1shnJT8NCY</td>\n      <td>-0.265967</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=[]\ny=[]\ncategory = []\nfor v in videos:\n    row=[]\n    temp_df = US3[US3[\"video_id\"]==v].sort_values(by=\"trending_date1\")\n    #print (temp_df)\n    seq = temp_df.views[0:4].index\n        \n    for s in seq:\n        row.append(US3.iloc[s].values[2:3])\n    x.append(row)\n    nextstep = temp_df.views[4:5].values\n    y.append(nextstep)","execution_count":27,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = np.reshape(x,(len(x),4,1))\nprint (x.shape)\nprint (x[0])","execution_count":28,"outputs":[{"output_type":"stream","text":"(3984, 4, 1)\n[[-0.2719427408449598]\n [-0.2555659799132111]\n [-0.24286315271238587]\n [-0.23675864020197335]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"y = np.reshape(y,(-1,1))\nprint (y.shape)\nprint (y[0])","execution_count":29,"outputs":[{"output_type":"stream","text":"(3984, 1)\n[-0.23191164]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"x = x.astype('float32')\ny = y.astype('float32')\n\n# Train,validation and test data split. Each one should be integral multiples of batchsize.\nbatch_size = 100\n\nx_train,x_remain = x[:3700],x[3700:3900]\ny_train,y_remain = y[:3700],y[3700:3900]\n\nx_val,x_test = x_remain[:100],x_remain[100:]\ny_val,y_test = y_remain[:100],y_remain[100:]\nprint (x_train.shape,x_val.shape,x_test.shape)","execution_count":30,"outputs":[{"output_type":"stream","text":"(3700, 4, 1) (100, 4, 1) (100, 4, 1)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch\nfrom torch.utils.data import TensorDataset,DataLoader","execution_count":31,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = TensorDataset(torch.from_numpy(x_train),torch.from_numpy(y_train))\nvalid_data = TensorDataset(torch.from_numpy(x_val),torch.from_numpy(y_val))\ntest_data = TensorDataset(torch.from_numpy(x_test),torch.from_numpy(y_test))\n\n\ntrain_loader = DataLoader(train_data,shuffle=True,batch_size=batch_size)\nvalid_loader = DataLoader(valid_data,shuffle=True,batch_size=batch_size)\ntest_loader = DataLoader(test_data,shuffle=True,batch_size=batch_size)","execution_count":35,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"dataiter = iter(train_loader)\nsample_x,sample_y = dataiter.next()\n\nprint ('Sample input size:',sample_x.size())\nprint ('Sample input:\\n',sample_x[0:2])\nprint ('\\n')\nprint ('Sample output size:',sample_y.size())\nprint ('Sample output:\\n',sample_y[0:2])","execution_count":36,"outputs":[{"output_type":"stream","text":"Sample input size: torch.Size([100, 4, 1])\nSample input:\n tensor([[[-0.2593],\n         [-0.2489],\n         [-0.2456],\n         [-0.2437]],\n\n        [[-0.3068],\n         [-0.3050],\n         [-0.3040],\n         [-0.3034]]])\n\n\nSample output size: torch.Size([100, 1])\nSample output:\n tensor([[-0.2418],\n        [-0.3027]])\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"<i>Now create the training model!</i>"},{"metadata":{"trusted":true},"cell_type":"code","source":"import torch.nn as nn\n\nclass mylstm(nn.Module):\n    \n    def __init__(self,input_size,output_size,hidden_dim, n_layers,drop_prob=0.5):\n        super(mylstm,self).__init__()\n        self.input_size = input_size\n        self.output_size = output_size\n        self.n_layers = n_layers\n        self.hidden_dim = hidden_dim\n        \n        self.lstm = nn.LSTM(input_size,hidden_dim,n_layers,batch_first=True)\n        self.dropout = nn.Dropout(0.3)\n        self.fc = nn.Linear(hidden_dim,output_size)\n        \n    def forward(self,x,hidden):\n        batch_size = x.size(0)\n        \n        lstm_out,hidden = self.lstm(x,hidden)\n        lstm_out = lstm_out.contiguous().view(-1, self.hidden_dim)\n        out =self.dropout(lstm_out)\n       \n        out = self.fc(out)#shape=(batchsize,seqlen,outputdim)\n        out = out.view(batch_size, -1) \n        out = out[:, -1]\n        return out,hidden\n    \n    def init_hidden(self,batch_size):\n        weight = next(self.parameters()).data\n        \n        if (train_on_gpu):\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda(),\n                  weight.new(self.n_layers, batch_size, self.hidden_dim).zero_().cuda())\n        else:\n            hidden = (weight.new(self.n_layers, batch_size, self.hidden_dim).zero_(),\n                      weight.new(self.n_layers, batch_size, self.hidden_dim).zero_())\n        \n        return hidden","execution_count":40,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"input_size = 1\noutput_size =1\nhidden_dim = 256\nn_layers = 2\n\nnet = mylstm(input_size,output_size,hidden_dim,n_layers)\nprint (net)","execution_count":41,"outputs":[{"output_type":"stream","text":"mylstm(\n  (lstm): LSTM(1, 256, num_layers=2, batch_first=True)\n  (dropout): Dropout(p=0.3, inplace=False)\n  (fc): Linear(in_features=256, out_features=1, bias=True)\n)\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_on_gpu = torch.cuda.is_available()\nif train_on_gpu:\n    print ('Training on GPU')\nelse:\n    print ('Training on CPU')","execution_count":43,"outputs":[{"output_type":"stream","text":"Training on CPU\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# Now train!\nlr = 0.001\nepoch = 40\ncounter = 0\nprint_every = 50\nclip = 5\n\ncriterion = nn.MSELoss()\noptimizer = torch.optim.Adam(net.parameters(),lr=lr)\n\nif train_on_gpu:\n    net.cuda()\n    \nnet.train()\nfor e in range(epoch):\n    h = net.init_hidden(batch_size)\n    for inputs,labels in train_loader:\n        counter +=1\n        if train_on_gpu:\n            inputs,labels = inputs.cuda(),output.cuda()\n            \n        h = tuple([each.data for each in h])\n        net.zero_grad()\n        output,h = net(inputs,h)\n        \n        loss = criterion(output.squeeze(),labels.squeeze())\n        loss.backward()\n        \n        nn.utils.clip_grad_norm_(net.parameters(),clip)\n        optimizer.step()\n        \n        if counter % print_every==0:\n            val_h = net.init_hidden(batch_size)\n            val_losses = []\n            net.eval()\n            for inputs,labels in valid_loader:\n                val_h = tuple([each.data for each in val_h])\n                if train_on_gpu:\n                    inputs,labels = inputs.cuda(),labels.cuda()\n                output,val_h = net(inputs,val_h)\n                val_loss = criterion(output.squeeze(),labels.squeeze())\n                \n                val_losses.append(val_loss.item())\n            net.train()\n            print (\"Epoch:{}/{}...\".format(e+1,epoch),\n               \"Step:{}...\".format(counter),\n               \"Loss:{:.6f}...\".format(loss.item()),\n               \"Val loss:{:.6f}\".format(np.mean(val_losses)))","execution_count":45,"outputs":[{"output_type":"stream","text":"Epoch:2/40... Step:50... Loss:0.059253... Val loss:0.009478\nEpoch:3/40... Step:100... Loss:0.114847... Val loss:0.069680\nEpoch:5/40... Step:150... Loss:0.005290... Val loss:0.003051\nEpoch:6/40... Step:200... Loss:0.002424... Val loss:0.007196\nEpoch:7/40... Step:250... Loss:0.003964... Val loss:0.001407\nEpoch:9/40... Step:300... Loss:0.004749... Val loss:0.003351\nEpoch:10/40... Step:350... Loss:0.001159... Val loss:0.001201\nEpoch:11/40... Step:400... Loss:0.000471... Val loss:0.000984\nEpoch:13/40... Step:450... Loss:0.002015... Val loss:0.001076\nEpoch:14/40... Step:500... Loss:0.010099... Val loss:0.009626\nEpoch:15/40... Step:550... Loss:0.000590... Val loss:0.000712\nEpoch:17/40... Step:600... Loss:0.000682... Val loss:0.001411\nEpoch:18/40... Step:650... Loss:0.002139... Val loss:0.000706\nEpoch:19/40... Step:700... Loss:0.000856... Val loss:0.001039\nEpoch:21/40... Step:750... Loss:0.004292... Val loss:0.002492\nEpoch:22/40... Step:800... Loss:0.000676... Val loss:0.000441\nEpoch:23/40... Step:850... Loss:0.004948... Val loss:0.001376\nEpoch:25/40... Step:900... Loss:0.001419... Val loss:0.001648\nEpoch:26/40... Step:950... Loss:0.000736... Val loss:0.002296\nEpoch:28/40... Step:1000... Loss:0.001224... Val loss:0.000268\nEpoch:29/40... Step:1050... Loss:0.013837... Val loss:0.005129\nEpoch:30/40... Step:1100... Loss:0.001016... Val loss:0.000405\nEpoch:32/40... Step:1150... Loss:0.003177... Val loss:0.000294\nEpoch:33/40... Step:1200... Loss:0.000868... Val loss:0.000213\nEpoch:34/40... Step:1250... Loss:0.003826... Val loss:0.000251\nEpoch:36/40... Step:1300... Loss:0.001191... Val loss:0.003740\nEpoch:37/40... Step:1350... Loss:0.002554... Val loss:0.000966\nEpoch:38/40... Step:1400... Loss:0.002394... Val loss:0.001994\nEpoch:40/40... Step:1450... Loss:0.003626... Val loss:0.001003\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_losses=[]\n\nh = net.init_hidden(batch_size)\nnet.eval()\nfor inputs,labels in test_loader:\n    h = tuple([each.data for each in h])\n    if train_on_gpu:\n        inputs,labels =inputs.cuda(),labels.cuda()\n    output,h = net(inputs,h)\n    test_loss = criterion(output.squeeze(),labels.squeeze())\n    \n    test_losses.append(test_loss.item())\n\nprint (\"Test loss is : {:.6f}\".format(np.mean(test_losses)))","execution_count":47,"outputs":[{"output_type":"stream","text":"Test loss is : 0.000831\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch import Tensor \nfrom sklearn.metrics import mean_squared_error\nimport math\npredict_tensor = torch.from_numpy(x[3900:3910])\nbatch_size = predict_tensor.size(0)\nh = net.init_hidden(batch_size)\nif train_on_gpu:\n    predict_tensor = predict_tensor.cuda()\noutput,h = net(predict_tensor,h)\npred = scaler_views.inverse_transform(Tensor.detach(output) )\ntrue = scaler_views.inverse_transform(y[3900:3910])\nfor i in range(10):\n    print (\"The true value is {}, and the predict value is {}\".format(true[i],pred[i]))\nprint (\"The RMSE : \",math.sqrt(mean_squared_error(pred,true)))","execution_count":50,"outputs":[{"output_type":"stream","text":"The true value is [3017144.], and the predict value is 3154952.0\nThe true value is [237230.97], and the predict value is 246670.46875\nThe true value is [460125.97], and the predict value is 472240.96875\nThe true value is [1899969.], and the predict value is 1961960.375\nThe true value is [879709.94], and the predict value is 867860.3125\nThe true value is [1827806.], and the predict value is 1873080.375\nThe true value is [96253.21], and the predict value is 97961.7109375\nThe true value is [351798.97], and the predict value is 370323.46875\nThe true value is [99200.96], and the predict value is 98440.7109375\nThe true value is [608431.06], and the predict value is 622209.9375\nThe RMSE :  50790.3033265209\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print (\"The average percentage error 10 data is {:.3f}%\".format(np.mean((pred-true)/true)))","execution_count":52,"outputs":[{"output_type":"stream","text":"The average percentage error 10 data is 2.309%\n","name":"stdout"}]}],"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.7.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":4}